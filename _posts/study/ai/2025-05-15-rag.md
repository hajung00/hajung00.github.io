---
title: RAG란 무엇인가? 검색 기반 생성(Retrieval-Augmented Generation)의 이해
author: cotes
date: 2025-05-15 11:33:00 +0800
categories: [Study, AI]
tags: [AI, RAG]
---

최근 AI와 자연어 처리(NLP) 분야에서는 **RAG(Retrieval-Augmented Generation)**이 큰 주목을 받고 있습니다. 단순한 질문 응답 시스템을 넘어서, 신뢰성 있는 생성 AI를 구축하기 위한 핵심 기술로 자리잡고 있죠.

이 글에서는 RAG의 개념부터 도입 배경, 동작 방식, 검색기 종류, 그리고 진화된 형태까지 자세하게 설명해보겠습니다.

<br/>


> ## RAG란?

**RAG(Retrieval-Augmented Generation)**은 대규모 언어 모델(LLM)에 검색 기능을 결합한 생성 방식입니다.

전통적인 LLM은 질문을 받으면 모델이 학습한 내부 지식만을 바탕으로 답변을 생성합니다. 하지만 학습 데이터에 없는 정보는 잘못된 사실로 채워지거나, 최신 정보를 반영할 수 없다는 한계가 있습니다.

**RAG는 외부의 문서나 지식 베이스에서 정보를 검색(Retrieval)**하여, 이를 바탕으로 **정확하고 근거 있는 응답을 생성(Generation)**합니다.

✅ 즉, RAG는 단순한 텍스트 생성이 아닌 문서 기반으로 정답을 "찾아" 말할 수 있는 AI를 만드는 방법입니다.

<br/>

---

<br/>

> ## RAG 도입 배경

LLM의 강력한 성능에도 불구하고 다음과 같은 한계들이 존재합니다:

🔸 1. 최신 정보 부족

GPT 같은 모델은 특정 시점까지의 데이터를 기반으로 학습됩니다. 그 이후의 사건이나 내용은 반영되지 않습니다.

🔸 2. 도메인 특화 지식의 부재

LLM은 일반적인 지식에 강하지만, 법률, 금융, 의료, 사내 문서처럼 특수한 분야의 정보는 잘 다루지 못합니다.

🔸 3. 할루시네이션(hallucination)

모델이 그럴듯하지만 사실이 아닌 정보를 만들어내는 문제가 발생합니다. 이는 특히 산업 현장에선 치명적일 수 있죠.

이러한 문제들을 해결하기 위해, LLM에 외부 지식(문서)을 결합한 RAG 구조가 등장하게 된 것입니다.

<br/>

---

<br/>

> ## RAG 사용 목적

RAG를 도입하는 이유는 다양하지만, 핵심 목적은 다음과 같습니다:

✅ 1. 정확도 향상

외부 문서를 검색하여 답변에 근거(context)를 부여함으로써 사실 기반의 응답 생성이 가능합니다.

✅ 2. 최신성 확보

검색을 통해 최신 뉴스, 변경된 정책, 업데이트된 데이터를 즉시 활용할 수 있습니다.

✅ 3. 도메인 확장성

전문가 영역(법률, 의료 등)의 문서를 넣어두면, LLM이 해당 분야 질문에 대한 응답 능력을 갖추게 됩니다.

✅ 4. 비용 효율성

모든 지식을 모델에 학습시키지 않고도, 검색 기반으로 확장할 수 있어 모델 사이즈와 학습 비용을 절약할 수 있습니다.

<br/>

---

<br/>

> ## RAG 기본 동작 구조

RAG는 두 개의 주요 흐름으로 구성됩니다:
① 문서 사전 처리 단계와 ② 사용자 질의 처리 단계입니다.

### 📁 1. 문서 사전 처리 (지식 베이스 구축)

![Image](https://github.com/user-attachments/assets/1d1e7bc3-9841-41ab-8b6d-7ad6c42e1c9c)

- 문서 로드: 다양한 파일(PDF, Word, TXT 등)을 로딩하고 텍스트로 변환합니다.

- 청킹(Chunking): 문서를 의미 단위로 잘게 나눕니다. (예: 문단별, 문장 3개 단위 등)

- 임베딩(Embedding): 각 청크를 벡터화하여 의미를 수치로 표현합니다.
(예: OpenAI, Cohere, HuggingFace 임베딩 모델 사용)

- 벡터 DB에 저장: FAISS, Pinecone, Weaviate 등의 벡터 DB에 저장합니다.

<br/>

### 💬 2. 사용자 질의 처리

![Image](https://github.com/user-attachments/assets/65d22144-e786-4ee4-a3bd-edd6341bd9a4)

- 질문 임베딩: 사용자의 질문을 동일한 임베딩 모델로 벡터화합니다.

- 유사 문서 검색 (Top-K 검색): 벡터 DB에서 질문 벡터와 유사한 청크 K개를 찾습니다.

- 문서 + 질문을 LLM에 입력: 검색된 문서들과 함께 질문을 LLM에 전달하여 응답을 생성합니다.

⚠️ Top-K에서 K 값을 너무 작게 설정하면 LLM이 충분한 정보를 얻지 못해, 응답의 질이 낮아질 수 있습니다.
반면 너무 크게 하면 토큰 초과 혹은 노이즈가 발생할 수 있습니다. 적절한 조정이 필요합니다.

<br/>

---

<br/>

> ## RAG 검색기 종류 (Retriever)

문서를 찾는 **Retriever(검색기)**는 크게 두 가지로 나뉩니다.

🔹 Sparse Retriever (희소 검색기)

- 키워드 기반 전통 검색 방식 (TF-IDF, BM25)

- 빠르고 단순하지만 의미 유사성 반영이 어려움

- 예: Elasticsearch, Lucene

<br/>

🔹 Dense Retriever (밀집 검색기)

- 임베딩을 통한 의미 유사성 기반 검색

- 표현이 달라도 같은 의미의 문서를 잘 찾음

- 예: Sentence-BERT, OpenAI Embedding, Cohere 등

<br/>

🔸 Hybrid Retriever (혼합 방식)

- Sparse와 Dense를 결합하여 정확도 향상

- 예: BM25로 후보 문서 추출 → Dense로 reranking

<br/>

---

<br/>

RAG는 단순한 챗봇을 넘어서, **"정보에 기반한 AI 응답"**을 제공하는 강력한 방법론입니다.
도메인 문서 기반 QA, 사내 지식 검색, 의료/법률 문서 응답 시스템 등 다양한 분야에서 활용도가 매우 높습니다.
다음으로는 RAG의 발전 단계에 대해서 알아보겠습니다.
